





















  0%|          | 0/100 [00:45<?, ?it/s]Traceback (most recent call last):
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\mainExperiment.py", line 37, in <module>
    experiment.run(track_cfg=track_cfg,runs=args.num_runs, show_progress=args.show_progress, wandb_logging=args.wandb_logging, save_latest=args.save_latest, save_best=args.save_best)
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Analysis\Experiment.py", line 123, in run
    agent.train(env_maker = self.env, tracker =run_tracker, norm_sync_env = eval_env)
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Agents\PPO.py", line 262, in train
    abort_training = abort_training or tracker.add_unit(tm.STEPS, self.cfg.num_envs)
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Analysis\RunTracker.py", line 62, in add_unit
    self.eval_func()
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Analysis\Experiment.py", line 85, in log
    eval_result = eval_func(initial_agent, agent, eval_env)
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Analysis\ExperimentSetup.py", line 189, in experiment_eval_func
    eval_results.update(evaluate_agent(agent,env, **eval_args))
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Analysis\Evaluation.py", line 92, in evaluate_agent
    action = agent.get_action(obs, eval_mode = True)
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Agents\PPO.py", line 199, in get_action
    action = self.get_action_and_value_net(obs, get_value=False, deterministic=deterministic)["action"].squeeze(0).detach().numpy()
  File "C:\Users\Robin\Desktop\Uni\Semester 10\Masterarbeit\Implementierung\EnsembleAgents\Agents\PPO.py", line 116, in get_action_and_value_net
    action = (1 - phase_frac) * vals[0]["action"] + phase_frac * vals[-1]["action"] + torch.sum(torch.stack([vals[i]["action"] for i in range(1, self.cfg.phases - 1)]), dim=0)
TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'